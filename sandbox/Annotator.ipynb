{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjjB_R1-0JKE"
   },
   "source": [
    "### Mount Google Drive and install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29302,
     "status": "ok",
     "timestamp": 1596821169487,
     "user": {
      "displayName": "kdglider stemaway",
      "photoUrl": "",
      "userId": "14511763866677580222"
     },
     "user_tz": 240
    },
    "id": "WfsZ7x2snpEX",
    "outputId": "ce5d690a-1968-4617-d374-d3b95f8806e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting import_ipynb\n",
      "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
      "Building wheels for collected packages: import-ipynb\n",
      "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=39d22cb39d27995e1e8afb6fabfc67fdd1e9a7d5483d0267db5afa3fa84bf652\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
      "Successfully built import-ipynb\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.3\n",
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Github/ml-team1-july2020\n"
     ]
    }
   ],
   "source": [
    "!pip install import_ipynb\n",
    "\n",
    "from google.colab import drive\n",
    "from os.path import join\n",
    "\n",
    "# Mounting location on runtime for GDrive\n",
    "ROOT = '/content/drive'\n",
    "\n",
    "# Mount GDrive on the runtime\n",
    "drive.mount(ROOT)\n",
    "\n",
    "# Create and change directory to workspace folder\n",
    "WORKING_PATH = '/content/drive/My Drive/Github/ml-team1-july2020'\n",
    "%cd {WORKING_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BsOJZeHMtJM"
   },
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1596821226555,
     "user": {
      "displayName": "kdglider stemaway",
      "photoUrl": "",
      "userId": "14511763866677580222"
     },
     "user_tz": 240
    },
    "id": "Y2rqfI_jMrhJ",
    "outputId": "127767b9-f477-46e2-a1e8-9da934ffee44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n# Import component classes\\nfrom TagPredictor import TagPredictor\\nfrom ManualTagger import ManualTagger\\nfrom classifier_NB import Classifier_NB\\nfrom classifier_SVM import Classifier_SVM\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/sandbox/TagPredictor')\n",
    "sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/sandbox/ManualTagger')\n",
    "\n",
    "# Import component notebooks in other folders\n",
    "import import_ipynb\n",
    "from sandbox.TagPredictor.TagPredictor import TagPredictor\n",
    "from sandbox.ManualTagger.ManualTagger import ManualTagger\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "'''\n",
    "# Import component classes\n",
    "from TagPredictor import TagPredictor\n",
    "from ManualTagger import ManualTagger\n",
    "from classifier_NB import Classifier_NB\n",
    "from classifier_SVM import Classifier_SVM\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPwHSTJ04Ud4"
   },
   "source": [
    "### Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-bLE6Q532mo"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@file       Annotator.ipynb\n",
    "@date       2020/08/03\n",
    "@brief      Top level class that defines the annotation tool and active learning algorithm\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "@brief  NLP classification annotation tool\n",
    "'''\n",
    "class Annotator:\n",
    "    labeledDB = None                # Pandas dataframe of labeled data\n",
    "    unlabelDB = None                # Pandas dataframe of unlabeled data\n",
    "\n",
    "    tagPredictor = None             # TagPredictor object\n",
    "    manualTagger = None             # ManualTagger object\n",
    "\n",
    "    confidenceThreshold = 0.8       # Prediction confidence threshold to determine if a topic should be passed to ManualTagger\n",
    "\n",
    "\n",
    "    def __init__(self, labeledDatafile, unlabeledDatafile, manualTagger):\n",
    "        # Create labeled and unlabeled databases\n",
    "        self.labeledDB, self.unlabeledDB = self.createDatabases(labeledDatafile, unlabeledDatafile)\n",
    "\n",
    "        # Set up ManualTagger\n",
    "        manualTagger = manualTagger\n",
    "    \n",
    "\n",
    "    '''\n",
    "    @brief      Performs preprocessing and cleaning on a sentence\n",
    "    @param      text    String that contains the raw sentence\n",
    "    @return     text    String that contains the cleaned sentence\n",
    "    '''\n",
    "    def cleanText(self, text):\n",
    "        ## Change all instance of featureString to text\n",
    "\n",
    "        # Replace newline and tab characters with spaces\n",
    "        featureString = featureString.replace('\\n', ' ')\n",
    "        featureString = featureString.replace('\\t', ' ')\n",
    "\n",
    "        # Convert all letters to lowercase\n",
    "        featureString = featureString.lower()\n",
    "        \n",
    "        # Strip all punctuation\n",
    "        #table = str.maketrans('', '', string.punctuation)\n",
    "        #featureString = featureString.translate(table)\n",
    "\n",
    "        # Remove all non-ASCII characters\n",
    "        #featureString = featureString.encode(encoding='ascii', errors='ignore').decode('ascii')\n",
    "\n",
    "        # Split feature string into a list to perform processing on each word\n",
    "        wordList = featureString.split()\n",
    "\n",
    "        # Remove all stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        wordList = [word for word in wordList if not word in stop_words]\n",
    "\n",
    "        # Remove all words to contain non-ASCII characters\n",
    "        wordList = [word for word in wordList if is_ascii(word)]\n",
    "\n",
    "        # Remove all leading/training punctuation, except for '$'\n",
    "        punctuation = '!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        wordList = [word.strip(punctuation) for word in wordList]\n",
    "\n",
    "        # Replace all numbers with ######## identifier\n",
    "        # Replace all costs with $$$$$$$$ identifier\n",
    "        wordList = ['########' if (word.replace('.','').isdigit()) \\\n",
    "                    else '$$$$$$$$' if (word.replace('.','').replace('$','').isdigit()) \\\n",
    "                    else word \\\n",
    "                    for word in wordList]\n",
    "        #wordList = ['########' if (word.replace('.','').isdigit()) else word for word in wordList]\n",
    "        #wordList = ['########' if (word.translate(table).isdigit()) else word for word in wordList]\n",
    "\n",
    "        # Reconstruct featureString\n",
    "        # If it is empty, do not add this sample to the final output\n",
    "        featureString = ' '.join(wordList)\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    '''\n",
    "    @brief      Loads data from CSV files into Pandas dataframes and performs cleanText() on all columns\n",
    "    @param      labeledDatafile     Labeled data CSV file\n",
    "    @param      unlabeledDatafile   Unlabeled data CSV file\n",
    "    @return     labeledDB           Pandas dataframe of the labeled data\n",
    "    @return     unlabeledDB         Pandas dataframe of the unlabeled data\n",
    "    '''\n",
    "    def createDatabases(self, labeledDatafile, unlabeledDatafile):\n",
    "        # Load CSV files as Pandas dataframes\n",
    "        labeledDB = pd.read_csv(labeledDatafile)\n",
    "        unlabeledDB = pd.read_csv(unlabeledDatafile)\n",
    "        # Combine topic title and leading comment columns\n",
    "        unlabeledDB['Bag_of_words'] = unlabeledDB['Topic Title'] + unlabeledDB['Leading Comment']\n",
    "        unlabeledDB['Bag_of_words'] = unlabeledDB['Bag_of_words'].str.strip().str.replace('   ', ' ').str.replace('  ', ' ')\n",
    "        \n",
    "        labeledDB['Bag_of_words'] = labeledDB['Topic Title'] + labeledDB['Leading Comment']\n",
    "        labeledDB['Bag_of_words'] = labeledDB['Bag_of_words'].str.strip().str.replace('   ', ' ').str.replace('  ', ' ')\n",
    "        # Apply cleanText() to all columns with this:\n",
    "        unlabeledDB['Bag_of_words'] = unlabeledDB['Bag_of_words'].apply(lambda x: cleanText(x))\n",
    "        labeledDB['Bag_of_words'] = labeledDB['Bag_of_words'].apply(lambda x: cleanText(x))\n",
    "        \n",
    "        return labeledDB, unlabeledDB\n",
    "\n",
    "\n",
    "    '''\n",
    "    @brief      Demonstration function to run the entire annotator application\n",
    "    @param      \n",
    "    @return     None\n",
    "    '''\n",
    "    def runApplication(self, classifier):\n",
    "        # Create labeled and unlabeled databases\n",
    "        self.labeledDB, self.unlabeledDB = self.createDatabases(labeledDatafile, unlabeledDatafile)\n",
    "\n",
    "        # Set up TagPredictor object\n",
    "        tagPredictor = TagPredictor(classifier, self.labeledDB)\n",
    "\n",
    "        # Train tagPredictor\n",
    "        tagPredictor.train()\n",
    "\n",
    "        # Predict tags for all unlabeled topics\n",
    "        tagList, confidenceList = tagPredictor.predict(self.unlabeledDB)\n",
    "\n",
    "        # Continue running the active learning loop as long as there are still low-confidence topics\n",
    "        while (any(p < self.confidenceThreshold for p in confidenceList) == True):\n",
    "            # Log tagging statistics\n",
    "            \n",
    "            # Get low-confidence topic indices\n",
    "            lowConfIndices = [i for i in range(len(L)) if confidenceList[i] < self.confidenceThreshold]\n",
    "\n",
    "            # Pass low-confidence topics to the manual tagger\n",
    "            lowConfTopics = self.unlabelDB.iloc(lowConfIndices)\n",
    "            labeledTopics = self.manualTagger.run(lowConfTopics)\n",
    "\n",
    "            # Add manually tagged topics to the labeled database\n",
    "            self.labeledDB = pd.concat([self.labeledDB, labeledTopics], join='inner')\n",
    "\n",
    "            # Remove tagged topics from unlabeled database\n",
    "            self.unlabeledDB = self.unlabeledDB.drop(lowConfTopics)\n",
    "\n",
    "            # Train tagPredictor with updated database\n",
    "            tagPredictor = TagPredictor(classifier, self.labeledDB)\n",
    "            tagPredictor.train()\n",
    "\n",
    "            # Predict tags for all unlabeled topics\n",
    "            tagList, confidenceList = tagPredictor.predict(self.unlabeledDB)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nb = Classifier_NB()\n",
    "    svm = Classifier_SVM()\n",
    "\n",
    "    # Set up Manual Tagger with ground truth database\n",
    "    #...\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMcvEU689DdPIJjvXoxIZul",
   "collapsed_sections": [],
   "name": "Annotator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
