{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AnnotatorPrototype.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DjjB_R1-0JKE"},"source":["### Mount Google Drive"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WfsZ7x2snpEX","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598330204088,"user_tz":240,"elapsed":330,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"24c679a6-f388-474a-cbf7-c54b1ead4873"},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create and change directory to workspace folder\n","WORKING_PATH = '/content/drive/My Drive/Github/ml-team1-july2020'\n","%cd {WORKING_PATH}"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Github/ml-team1-july2020\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_BsOJZeHMtJM"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y2rqfI_jMrhJ","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598330224116,"user_tz":240,"elapsed":2763,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"28245980-3543-4619-bc5d-d355cf44454d"},"source":["# Add other folders to the system path\n","import sys\n","sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/sandbox/TagPredictor')\n","sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/sandbox/ManualTagger')\n","\n","# Import component classes in other folders\n","#from sandbox.TagPredictor.classifier import Classifier\n","#from sandbox.TagPredictor.classifier_SVM import Classifier_SVM\n","from sandbox.TagPredictor.multilabelclassifier_SVM import MultilabelClassifier_SVM\n","from sandbox.TagPredictor.TagPredictor import TagPredictor\n","from sandbox.ManualTagger.ManualTagger import ManualTagger\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import re\n","import ast\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import hamming_loss\n","from sklearn.metrics import accuracy_score\n","\n","# Set Pandas display options\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', 20)\n","pd.set_option('display.width', None)\n","pd.set_option('display.expand_frame_repr', False)   # Disable wrapping\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vPwHSTJ04Ud4"},"source":["### Class Definition"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7-bLE6Q532mo","colab":{},"executionInfo":{"status":"ok","timestamp":1598330224617,"user_tz":240,"elapsed":492,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}}},"source":["'''\n","@file       Annotator.ipynb\n","@date       2020/08/03\n","@brief      Top level class that defines the annotation tool and active learning algorithm\n","'''\n","\n","\n","'''\n","@brief  NLP classification annotation tool\n","'''\n","class Annotator:\n","    groundTruthDB = None            # Pandas dataframe of all data with ground truth labels\n","    labeledDB = None                # Pandas dataframe of labeled data\n","    unlabeledDB = None              # Pandas dataframe of unlabeled data\n","\n","    tagPredictor = None             # TagPredictor object\n","    manualTagger = None             # ManualTagger object\n","\n","    confidenceThreshold = 0.95      # Prediction confidence threshold to determine if a topic should be passed to ManualTagger\n","\n","\n","    def __init__(self, datafile):\n","        # Create databases\n","        self.groundTruthDB, self.labeledDB, self.unlabeledDB = self.createDatabases(datafile)\n","\n","        # Set up ManualTagger\n","        self.manualTagger = ManualTagger(self.groundTruthDB)\n","    \n","\n","    '''\n","    @brief      Performs preprocessing and cleaning on a sentence\n","    @param      text    String that contains the raw sentence\n","    @return     text    String that contains the cleaned sentence\n","    '''\n","    def cleanText(self, text):\n","        # Function that checks if all characters in a string are ASCII\n","        def is_ascii(s):\n","            return all(ord(c) < 128 for c in s)\n","        \n","        # Remove URLs\n","        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n","\n","        # Replace newline and tab characters with spaces\n","        text = text.replace('\\n', ' ')\n","        text = text.replace('\\t', ' ')\n","\n","        # Convert all letters to lowercase\n","        text = text.lower()\n","\n","        # Split feature string into a list to perform processing on each word\n","        wordList = text.split()\n","\n","        # Remove all stop words\n","        #stop_words = set(stopwords.words('english'))\n","        #wordList = [word for word in wordList if not word in stop_words]\n","\n","        # Remove all words to contain non-ASCII characters\n","        wordList = [word for word in wordList if is_ascii(word)]\n","\n","        # Remove all leading/training punctuation, except for '$'\n","        punctuation = '!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","        wordList = [word.strip(punctuation) for word in wordList]\n","\n","        # Reconstruct text\n","        text = ' '.join(wordList)\n","\n","        return text\n","\n","\n","    '''\n","    @brief      Loads data from CSV files into Pandas dataframes and performs cleanText() on all columns\n","    @param      datafile        CSV file with all data\n","    @return     groundTruthDB   Pandas dataframe of all data with ground truth labels\n","    @return     labeledDB       Pandas dataframe of the labeled data\n","    @return     unlabeledDB     Pandas dataframe of the unlabeled data\n","    '''\n","    def createDatabases(self, datafile):\n","        # Load CSV file as ground truth database\n","        groundTruthDB = pd.read_csv(datafile)\n","\n","        # Combine topic title and leading comment columns\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Topic Title'] + groundTruthDB['Leading Comment']\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Bag_of_Words'].str.strip().str.replace('   ', ' ').str.replace('  ', ' ')\n","\n","        # Delete unused columns\n","        groundTruthDB = groundTruthDB.drop(columns=['Topic Title', 'Leading Comment', 'Unnamed: 0'])\n","        \n","        # Apply cleanText() to the bag of words\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Bag_of_Words'].apply(lambda x: self.cleanText(x))\n","\n","        # Code to duplicate multi-tag topics if necessary\n","        '''\n","        #create an offset value\n","        offset = 0\n","        #the total number of unique comments\n","        total = len(groundTruthDB)\n","        for index, entry in enumerate(groundTruthDB['Bag_of_Words']):\n","            #create a duplicate if post has multiple tags\n","            tag_list = ast.literal_eval(groundTruthDB.loc[index, 'Tags'])\n","            text = groundTruthDB.loc[index,'Bag_of_Words']\n","            while (isinstance(tag_list, list) and len(tag_list) > 1):\n","                #print(index)\n","                #sets the tag for the duplicate to a string\n","                groundTruthDB.loc[total+offset, 'Tags'] = tag_list.pop()\n","                #Adds the duplicate to the end of the pandas dataframe\n","                groundTruthDB.loc[total+offset, 'Bag_of_Words'] = text\n","                offset = offset + 1\n","            #Changes the first tag to a string\n","            if (len(tag_list) == 1):\n","                groundTruthDB.loc[index, 'Tags'] = tag_list.pop()\n","            #Changes empty tags from lists to strings\n","            if (isinstance(groundTruthDB.loc[index, 'Tags'], list)):\n","                groundTruthDB.loc[index, 'Tags'] = ''\n","                # Not sure why this element is stored as '[]' instead of ''\n","        '''\n","\n","        # Filter out topics with no tags\n","        groundTruthDB = groundTruthDB[groundTruthDB['Tags'].map(len) > 2]\n","\n","        # Convert Tag column elements from strings to lists\n","        groundTruthDB['Tags'] = groundTruthDB.Tags.apply(lambda x: x[1:-1].split(','))\n","\n","        # Take only a subset of the full dataset, if necessary\n","        groundTruthDB = groundTruthDB.sample(2000)\n","\n","        # Split ground truth database into labeled and unlabelled databases\n","        unlabeledDB, labeledDB = train_test_split(groundTruthDB, test_size=0.2)\n","\n","        return groundTruthDB, labeledDB, unlabeledDB\n","\n","\n","    '''\n","    @brief      Demonstration function to run the entire annotator application\n","    @param      \n","    @return     None\n","    '''\n","    def runApplication(self, classifier):\n","        # Create multilabel binarizer for metric calculations\n","        mlb = MultiLabelBinarizer()\n","\n","        # Set up TagPredictor object\n","        tagPredictor = TagPredictor(classifier, self.labeledDB)\n","\n","        # Train tagPredictor\n","        tagPredictor.train()\n","\n","        # Predict tags for all unlabeled topics\n","        tagList, confidenceList = tagPredictor.predict(self.unlabeledDB['Bag_of_Words'])\n","\n","        # Continue running the active learning loop as long as there are still low-confidence topics\n","        counter = 1\n","        print('Minimum Confidence: ', min(confidenceList))\n","        print('Maximum Confidence: ', max(confidenceList))\n","        while (any(p < self.confidenceThreshold for p in confidenceList) == True):\n","            # Print out active learning statistics\n","            print('Active Learning Iteration ', counter)\n","            print('Labeled Database Size: ', len(self.labeledDB))\n","            print('Unlabeled Database Size: ', len(self.unlabeledDB))\n","            trueLabelIndicatorMatrix = mlb.fit_transform(self.unlabeledDB['Tags'])\n","            predictedLabelIndicatorMatrix = mlb.transform(tagList)\n","            print('Hamming Loss: ', hamming_loss(trueLabelIndicatorMatrix, predictedLabelIndicatorMatrix))\n","            print('Accuracy: ', accuracy_score(trueLabelIndicatorMatrix, predictedLabelIndicatorMatrix))\n","            \n","            # Get low-confidence topic indices\n","            lowConfIndices = [i for i in range(len(confidenceList)) if confidenceList[i] < self.confidenceThreshold]\n","\n","            # Pass low-confidence topics to the manual tagger\n","            lowConfTopics = self.unlabeledDB.iloc[lowConfIndices]\n","            labeledTopics = self.manualTagger.run(lowConfTopics)\n","\n","            # Add manually tagged topics to the labeled database\n","            self.labeledDB = pd.concat([self.labeledDB, labeledTopics], join='inner')\n","\n","            # Remove tagged topics from unlabeled database\n","            cond = self.unlabeledDB['Bag_of_Words'].isin(lowConfTopics['Bag_of_Words'])\n","            print(len(self.unlabeledDB))\n","            print(len(lowConfTopics))\n","            self.unlabeledDB.drop(self.unlabeledDB[cond].index, inplace=True)\n","\n","            # Exit active learning loop if there are no more topics in the unlabeled database\n","            if (len(self.unlabeledDB) == 0):\n","                break\n","\n","            # Train tagPredictor with updated labeled database\n","            tagPredictor = TagPredictor(classifier, self.labeledDB)\n","            tagPredictor.train()\n","\n","            # Predict tags for all unlabeled topics\n","            tagList, confidenceList = tagPredictor.predict(self.unlabeledDB['Bag_of_Words'])\n","\n","            counter += 1\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziCVPa5-pK_0","colab_type":"text"},"source":["### Main"]},{"cell_type":"code","metadata":{"id":"J-DFjkzFpIZM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1598330231101,"user_tz":240,"elapsed":2461,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"a70b2c3c-3446-4546-adc6-d421f38b7dc6"},"source":["# Path to CSV datafile\n","datafile = '/content/drive/My Drive/Github/ml-team1-july2020/sandbox/Webscraper/StackOverflow_new_tags.csv'\n","\n","# Instantiate Annotator object\n","annotator = Annotator(datafile)\n","\n","# Run annotation application\n","annotator.runApplication(MultilabelClassifier_SVM)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Started training\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6ff92bf9f99c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Run annotation application\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mannotator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunApplication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultilabelClassifier_SVM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-796df943577a>\u001b[0m in \u001b[0;36mrunApplication\u001b[0;34m(self, classifier)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Train tagPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mtagPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Predict tags for all unlabeled topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Github/ml-team1-july2020/sandbox/TagPredictor/TagPredictor.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_X_Tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'Train_Y'"]}]},{"cell_type":"markdown","metadata":{"id":"1clUyBhIpOgR","colab_type":"text"},"source":["### Test Code"]},{"cell_type":"code","metadata":{"id":"Nkgm3CT6hKEW","colab_type":"code","colab":{}},"source":["# Set up TagPredictor object\n","tagPredictor = TagPredictor(MultilabelClassifier_SVM, annotator.labeledDB)\n","\n","# Train tagPredictor\n","tagPredictor.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mEFR66ZImCGN","colab_type":"code","colab":{}},"source":["# Predict tags for all unlabeled topics\n","tagList, confidenceList = tagPredictor.predict(annotator.unlabeledDB['Bag_of_Words'])\n","print(tagList)\n","print(confidenceList)"],"execution_count":null,"outputs":[]}]}