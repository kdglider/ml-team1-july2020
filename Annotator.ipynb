{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Annotator.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DjjB_R1-0JKE"},"source":["### Mount Google Drive"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WfsZ7x2snpEX","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1598919356806,"user_tz":240,"elapsed":24050,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"851084f4-df12-4faa-86b1-4785e4cff061"},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create and change directory to workspace folder\n","WORKING_PATH = '/content/drive/My Drive/Github/ml-team1-july2020'\n","%cd {WORKING_PATH}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Github/ml-team1-july2020\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_BsOJZeHMtJM"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y2rqfI_jMrhJ","colab":{"base_uri":"https://localhost:8080/","height":379},"executionInfo":{"status":"error","timestamp":1598934040435,"user_tz":240,"elapsed":695,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"1701b773-ef2f-4efb-f989-411632621fec"},"source":["# Add other folders to the system path\n","import sys\n","sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/TagPredictor')\n","sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/ManualTagger')\n","\n","# Import component classes in other folders\n","#from TagPredictor.classifier import Classifier\n","#from TagPredictor.classifier_SVM import Classifier_SVM\n","from TagPredictor.MultilabelClassifier_SVM import MultilabelClassifier_SVM\n","from TagPredictor.TagPredictor import TagPredictor\n","from ManualTagger.ManualTagger import ManualTagger\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import re\n","import ast\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import hamming_loss\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","# Set Pandas display options\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', 20)\n","pd.set_option('display.width', None)\n","pd.set_option('display.expand_frame_repr', False)   # Disable wrapping\n"],"execution_count":14,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-2b7b415cc5ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#from TagPredictor.classifier import Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#from TagPredictor.classifier_SVM import Classifier_SVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mTagPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultilabelClassifier_SVM\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultilabelClassifier_SVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mTagPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagPredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTagPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mManualTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mManualTagger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mManualTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'TagPredictor.MultilabelClassifier_SVM'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vPwHSTJ04Ud4"},"source":["### Class Definition"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7-bLE6Q532mo","colab":{},"executionInfo":{"status":"ok","timestamp":1598929197760,"user_tz":240,"elapsed":641,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}}},"source":["'''\n","@file       Annotator.ipynb\n","@date       2020/08/03\n","@brief      Top level class that defines the annotation tool and active learning algorithm\n","'''\n","\n","\n","'''\n","@brief  NLP classification annotation tool\n","'''\n","class Annotator:\n","    groundTruthDB = None            # Pandas dataframe of all data with ground truth labels\n","    labeledDB = None                # Pandas dataframe of labeled data\n","    unlabeledDB = None              # Pandas dataframe of unlabeled data\n","\n","    tagPredictor = None             # TagPredictor object\n","    manualTagger = None             # ManualTagger object\n","\n","    confidenceThreshold = 0.85       # Prediction confidence threshold to determine if a topic should be passed to ManualTagger\n","\n","\n","    def __init__(self, datafile):\n","        # Create databases\n","        self.groundTruthDB, self.labeledDB, self.unlabeledDB = self.createDatabases(datafile)\n","\n","        # Set up ManualTagger\n","        self.manualTagger = ManualTagger(self.groundTruthDB)\n","    \n","\n","    '''\n","    @brief      Performs preprocessing and cleaning on a sentence\n","    @param      text    String that contains the raw sentence\n","    @return     text    String that contains the cleaned sentence\n","    '''\n","    def cleanText(self, text):\n","        # Function that checks if all characters in a string are ASCII\n","        def is_ascii(s):\n","            return all(ord(c) < 128 for c in s)\n","        \n","        # Remove URLs\n","        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n","\n","        # Replace newline and tab characters with spaces\n","        text = text.replace('\\n', ' ')\n","        text = text.replace('\\t', ' ')\n","\n","        # Convert all letters to lowercase\n","        text = text.lower()\n","\n","        # Split feature string into a list to perform processing on each word\n","        wordList = text.split()\n","\n","        # Remove all stop words\n","        #stop_words = set(stopwords.words('english'))\n","        #wordList = [word for word in wordList if not word in stop_words]\n","\n","        # Remove all words to contain non-ASCII characters\n","        wordList = [word for word in wordList if is_ascii(word)]\n","\n","        # Remove all leading/training punctuation, except for '$'\n","        punctuation = '!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","        wordList = [word.strip(punctuation) for word in wordList]\n","\n","        # Reconstruct text\n","        text = ' '.join(wordList)\n","\n","        return text\n","\n","\n","    '''\n","    @brief      Loads data from CSV files into Pandas dataframes and performs cleanText() on all columns\n","    @param      datafile        CSV file with all data\n","    @return     groundTruthDB   Pandas dataframe of all data with ground truth labels\n","    @return     labeledDB       Pandas dataframe of the labeled data\n","    @return     unlabeledDB     Pandas dataframe of the unlabeled data\n","    '''\n","    def createDatabases(self, datafile):\n","        # Load CSV file as ground truth database\n","        groundTruthDB = pd.read_csv(datafile)\n","\n","        # Combine topic title and leading comment columns\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Topic Title'] + groundTruthDB['Leading Comment']\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Bag_of_Words'].str.strip().str.replace('   ', ' ').str.replace('  ', ' ')\n","\n","        # Delete unused columns\n","        groundTruthDB = groundTruthDB.drop(columns=['Topic Title', 'Leading Comment', 'Unnamed: 0'])\n","        \n","        # Apply cleanText() to the bag of words\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Bag_of_Words'].apply(lambda x: self.cleanText(x))\n","\n","        # Code to duplicate multi-tag topics if necessary\n","        '''\n","        #create an offset value\n","        offset = 0\n","        #the total number of unique comments\n","        total = len(groundTruthDB)\n","        for index, entry in enumerate(groundTruthDB['Bag_of_Words']):\n","            #create a duplicate if post has multiple tags\n","            tag_list = ast.literal_eval(groundTruthDB.loc[index, 'Tags'])\n","            text = groundTruthDB.loc[index,'Bag_of_Words']\n","            while (isinstance(tag_list, list) and len(tag_list) > 1):\n","                #print(index)\n","                #sets the tag for the duplicate to a string\n","                groundTruthDB.loc[total+offset, 'Tags'] = tag_list.pop()\n","                #Adds the duplicate to the end of the pandas dataframe\n","                groundTruthDB.loc[total+offset, 'Bag_of_Words'] = text\n","                offset = offset + 1\n","            #Changes the first tag to a string\n","            if (len(tag_list) == 1):\n","                groundTruthDB.loc[index, 'Tags'] = tag_list.pop()\n","            #Changes empty tags from lists to strings\n","            if (isinstance(groundTruthDB.loc[index, 'Tags'], list)):\n","                groundTruthDB.loc[index, 'Tags'] = ''\n","                # Not sure why this element is stored as '[]' instead of ''\n","        '''\n","\n","        # Filter out topics with no tags\n","        groundTruthDB = groundTruthDB[groundTruthDB['Tags'].map(len) > 2]\n","\n","        # Convert Tag column elements from strings to lists\n","        groundTruthDB['Tags'] = groundTruthDB.Tags.apply(lambda x: x[1:-1].split(','))\n","\n","        # Take only a subset of the full dataset, if necessary\n","        #groundTruthDB = groundTruthDB.sample(1000)\n","\n","        # Split ground truth database into labeled and unlabelled databases\n","        unlabeledDB, labeledDB = train_test_split(groundTruthDB, test_size=0.2)\n","\n","        return groundTruthDB, labeledDB, unlabeledDB\n","\n","\n","    '''\n","    @brief      Demonstration function to run the entire annotator application\n","    @param      classifier      Scikit-learn like classifier class to be used\n","    @param      maxIterations   Maximum times to run the active learning loop\n","    @return     None\n","    '''\n","    def runApplication(self, classifier, maxIterations=8):\n","        # Create multilabel binarizer for metric calculations\n","        mlb = MultiLabelBinarizer()\n","\n","        # Set up dictionary to keep track of statistics\n","        statDict = {\n","                    'Active Learning Iteration' :   [],\n","                    'Labeled Database Size'     :   [],\n","                    'Unlabeled Database Size'   :   [],\n","                    'Precision (Tag)'           :   [],\n","                    'Recall (Tag)'              :   [],\n","                    'F1 Score (Tag)'            :   [],\n","                    'Hamming Loss'              :   [],\n","                    'Accuracy (Tag List)'       :   []}\n","\n","        # Set up TagPredictor object\n","        tagPredictor = TagPredictor(classifier, self.labeledDB)\n","\n","        # Train tagPredictor\n","        tagPredictor.train()\n","\n","        # Predict tags for all unlabeled topics\n","        tagList, confidenceList = tagPredictor.predict(self.unlabeledDB['Bag_of_Words'])\n","\n","        # Continue running the active learning loop as long as there are still low-confidence topics\n","        counter = 1\n","        print('Minimum Confidence:', min(confidenceList))\n","        print('Maximum Confidence:', max(confidenceList))\n","        while (any(p < self.confidenceThreshold for p in confidenceList) == True and counter <= maxIterations):\n","            ## Start of statistic logging\n","            labeledDBSize = len(self.labeledDB)\n","            unlabeledDBSize = len(self.unlabeledDB)\n","\n","            # Calculate Hamming Loss and tag accuracy at the tag list level\n","            trueLabelIndicatorMatrix = mlb.fit_transform(self.unlabeledDB['Tags'])\n","            predictedLabelIndicatorMatrix = mlb.transform(tagList)\n","            hammingLoss = hamming_loss(trueLabelIndicatorMatrix, predictedLabelIndicatorMatrix)\n","            accuracy = accuracy_score(trueLabelIndicatorMatrix, predictedLabelIndicatorMatrix)\n","\n","            # Calculate average precision, recall and F1 score at the tag level (not the tag list level)\n","            precisions = np.zeros(trueLabelIndicatorMatrix.shape[0])\n","            recalls = np.zeros(trueLabelIndicatorMatrix.shape[0])\n","            fscores = np.zeros(trueLabelIndicatorMatrix.shape[0])\n","            for i in range(trueLabelIndicatorMatrix.shape[0]):\n","                pArray, rArray, fArray, _ = precision_recall_fscore_support(trueLabelIndicatorMatrix[i], predictedLabelIndicatorMatrix[i])\n","                precisions[i] = pArray[1]\n","                recalls[i] = rArray[1]\n","                fscores[i] = fArray[1]\n","            precision = np.average(precisions)\n","            recall = np.average(recalls)\n","            fscore = np.average(fscores)\n","\n","            # Print out active learning statistics\n","            print('Active Learning Iteration:', counter)\n","            print('Labeled Database Size:', labeledDBSize)\n","            print('Unlabeled Database Size:', unlabeledDBSize)\n","            print('Precision:', precision)\n","            print('Recall:', recall)\n","            print('F1 Score:', fscore)\n","            print('Hamming Loss:', hammingLoss)\n","            print('Accuracy:', accuracy)\n","            \n","            # Append statistics to statDict\n","            statDict['Active Learning Iteration'].append(counter)\n","            statDict['Labeled Database Size'].append(labeledDBSize)\n","            statDict['Unlabeled Database Size'].append(unlabeledDBSize)\n","            statDict['Precision (Tag)'].append(precision)\n","            statDict['Recall (Tag)'].append(recall)\n","            statDict['F1 Score (Tag)'].append(fscore)\n","            statDict['Hamming Loss'].append(hammingLoss)\n","            statDict['Accuracy (Tag List)'].append(accuracy)\n","\n","            ## End of statistics logging\n","            \n","            # Get low-confidence topic indices\n","            lowConfIndices = [i for i in range(len(confidenceList)) if confidenceList[i] < self.confidenceThreshold]\n","\n","            # Pass low-confidence topics to the manual tagger\n","            lowConfTopics = self.unlabeledDB.iloc[lowConfIndices]\n","            labeledTopics = self.manualTagger.run(lowConfTopics)\n","\n","            # Add manually tagged topics to the labeled database\n","            self.labeledDB = pd.concat([self.labeledDB, labeledTopics], join='inner')\n","\n","            # Remove tagged topics from unlabeled database\n","            cond = self.unlabeledDB['Bag_of_Words'].isin(lowConfTopics['Bag_of_Words'])\n","            self.unlabeledDB.drop(self.unlabeledDB[cond].index, inplace=True)\n","\n","            # Exit active learning loop if there are no more topics in the unlabeled database\n","            if (len(self.unlabeledDB) == 0):\n","                break\n","\n","            # Train tagPredictor with updated labeled database\n","            tagPredictor = TagPredictor(classifier, self.labeledDB)\n","            tagPredictor.train()\n","\n","            # Predict tags for all unlabeled topics\n","            tagList, confidenceList = tagPredictor.predict(self.unlabeledDB['Bag_of_Words'])\n","\n","            counter += 1\n","        \n","        # Save statistics to a CSV file\n","        statDataframe =  pd.DataFrame(statDict)\n","        statDataframe.to_csv('Annotator_Statistics_2080_85_2.csv') \n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziCVPa5-pK_0","colab_type":"text"},"source":["### Main"]},{"cell_type":"code","metadata":{"id":"J-DFjkzFpIZM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598932833251,"user_tz":240,"elapsed":3633327,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"f38a47df-e564-4d89-afb9-0b9aeb2cee63"},"source":["# Path to CSV datafile\n","datafile = '/content/drive/My Drive/Github/ml-team1-july2020/datasets/Team1Dataset.csv'\n","\n","# Instantiate Annotator object\n","annotator = Annotator(datafile)\n","\n","# Run annotation application\n","annotator.runApplication(MultilabelClassifier_SVM)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Started training\n","Finished training\n","Minimum Confidence: 0.6102929099175769\n","Maximum Confidence: 0.9727306292783482\n","Active Learning Iteration: 1\n","Labeled Database Size: 1452\n","Unlabeled Database Size: 5804\n","Precision: 0.38100160808637723\n","Recall: 0.337942223753733\n","F1 Score: 0.34444635883298874\n","Hamming Loss: 0.06162858506070084\n","Accuracy: 0.22191592005513439\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 2\n","Labeled Database Size: 4142\n","Unlabeled Database Size: 3114\n","Precision: 0.43727253264825516\n","Recall: 0.3939199314921858\n","F1 Score: 0.4025797473774352\n","Hamming Loss: 0.05495034830294946\n","Accuracy: 0.3015414258188825\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 3\n","Labeled Database Size: 4835\n","Unlabeled Database Size: 2421\n","Precision: 0.46234338427646976\n","Recall: 0.4170797191243288\n","F1 Score: 0.42679333608701636\n","Hamming Loss: 0.05228290916023258\n","Accuracy: 0.32837670384138784\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 4\n","Labeled Database Size: 5032\n","Unlabeled Database Size: 2225\n","Precision: 0.46808988764044945\n","Recall: 0.4237078651685393\n","F1 Score: 0.4331385767790262\n","Hamming Loss: 0.05165082108902334\n","Accuracy: 0.3348314606741573\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 5\n","Labeled Database Size: 5105\n","Unlabeled Database Size: 2152\n","Precision: 0.4776177199504337\n","Recall: 0.4319625154894671\n","F1 Score: 0.44180607187112764\n","Hamming Loss: 0.050882899628252785\n","Accuracy: 0.34107806691449816\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 6\n","Labeled Database Size: 5144\n","Unlabeled Database Size: 2113\n","Precision: 0.47688909922700745\n","Recall: 0.43196876478939894\n","F1 Score: 0.44161539675027606\n","Hamming Loss: 0.05094834176708289\n","Accuracy: 0.34264079507808803\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 7\n","Labeled Database Size: 5155\n","Unlabeled Database Size: 2102\n","Precision: 0.477243894703457\n","Recall: 0.4308198541071995\n","F1 Score: 0.44132572153504596\n","Hamming Loss: 0.05094049623069604\n","Accuracy: 0.3434823977164605\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 8\n","Labeled Database Size: 5164\n","Unlabeled Database Size: 2093\n","Precision: 0.477384933906673\n","Recall: 0.4321946169772256\n","F1 Score: 0.44202898550724634\n","Hamming Loss: 0.050883898709985664\n","Accuracy: 0.34400382226469184\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1clUyBhIpOgR","colab_type":"text"},"source":["### Test Code"]},{"cell_type":"code","metadata":{"id":"Nkgm3CT6hKEW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1598848251633,"user_tz":240,"elapsed":2766,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"00326498-72b7-4c47-af84-14e537f3e8f6"},"source":["# Set up TagPredictor object\n","tagPredictor = TagPredictor(MultilabelClassifier_SVM, annotator.labeledDB)\n","\n","# Train tagPredictor\n","tagPredictor.train()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Started training\n","[\" 'beautifulsoup'\" \" 'nlp'\" \" 'nltk'\" \" 'scrapy'\" \" 'selenium-webdriver'\"\n"," \" 'sentiment-analysis'\" \" 'splinter'\" \" 'text-classification'\"\n"," \" 'text-mining'\" \" 'tf-idf'\" \" 'web-scraping'\" \" 'word-embedding'\"\n"," \"'beautifulsoup'\" \"'nlp'\" \"'nltk'\" \"'scikit-learn'\" \"'scrapy'\"\n"," \"'selenium'\" \"'selenium-webdriver'\" \"'sentiment-analysis'\" \"'splinter'\"\n"," \"'text-classification'\" \"'text-mining'\" \"'tf-idf'\" \"'web-scraping'\"\n"," \"'word-embedding'\"]\n","Finished training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mEFR66ZImCGN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1598848309031,"user_tz":240,"elapsed":2042,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"e0658fa2-a592-4f11-f19a-219a8ad66a09"},"source":["# Predict tags for all unlabeled topics\n","tagList, confidenceList = tagPredictor.predict(annotator.unlabeledDB['Bag_of_Words'])\n","print(tagList)\n","#print(confidenceList)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[(\"'scikit-learn'\",), (\"'text-mining'\",), (\"'tf-idf'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\"'nltk'\",), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'splinter'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'word-embedding'\", \"'tf-idf'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'splinter'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'selenium'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'splinter'\", \"'selenium'\"), (\"'selenium'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\"'nltk'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'text-classification'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'nltk'\", \"'text-mining'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\", \"'splinter'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\"'web-scraping'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\"'web-scraping'\",), (\"'selenium'\", \"'web-scraping'\"), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'selenium'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'selenium'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'web-scraping'\",), (\"'selenium'\",), (\"'nltk'\", \"'scikit-learn'\"), (\"'text-mining'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'web-scraping'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'splinter'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'scrapy'\", \"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'nltk'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'text-mining'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'selenium'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\"'scikit-learn'\",), (\" 'word-embedding'\",), (\"'scrapy'\", \"'web-scraping'\"), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'selenium'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'word-embedding'\", \"'word-embedding'\"), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'scikit-learn'\", \"'tf-idf'\"), (\"'text-mining'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'scikit-learn'\",), (\"'tf-idf'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'nltk'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\"'text-mining'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\", \"'text-classification'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'tf-idf'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'text-mining'\",), (\"'selenium'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'scikit-learn'\",), (\" 'scrapy'\", \"'web-scraping'\"), (\"'word-embedding'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'tf-idf'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'splinter'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'scrapy'\", \"'web-scraping'\"), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'scrapy'\", \"'scrapy'\", \"'web-scraping'\"), (\"'scikit-learn'\", \"'splinter'\"), (\"'selenium'\", \"'splinter'\"), (\"'scikit-learn'\", \"'splinter'\"), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'word-embedding'\",), (\"'text-mining'\", \"'tf-idf'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'web-scraping'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\", \"'text-mining'\"), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\", \"'web-scraping'\"), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\"'nltk'\", \"'text-mining'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'splinter'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'text-mining'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\", \"'web-scraping'\"), (\"'selenium'\",), (\"'web-scraping'\",), (\"'scrapy'\",), (\"'selenium'\",), (\"'selenium'\",), (\" 'text-mining'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\"'splinter'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'text-mining'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'scikit-learn'\", \"'tf-idf'\"), (\"'text-classification'\",), (\"'selenium'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'nltk'\", \"'text-mining'\"), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'text-mining'\",), (\"'nltk'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\", \"'tf-idf'\"), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\"'text-classification'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'web-scraping'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'selenium'\",), (\"'text-classification'\",), (\"'text-mining'\", \"'tf-idf'\"), (\"'selenium'\", \"'web-scraping'\"), (\"'selenium'\",), (\"'selenium'\",), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\"'scrapy'\", \"'web-scraping'\"), (\"'tf-idf'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'text-mining'\",), (\"'scrapy'\", \"'splinter'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'scrapy'\", \"'scrapy'\"), (\"'scikit-learn'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\", \"'word-embedding'\"), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'selenium'\", \"'web-scraping'\"), (\"'scrapy'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'text-mining'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'text-mining'\",), (\" 'splinter'\", \"'selenium'\"), (\"'word-embedding'\",), (\"'selenium'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'selenium'\", \"'web-scraping'\"), (\"'text-classification'\",), (\"'scrapy'\",), (\"'nltk'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",)]\n"],"name":"stdout"}]}]}