{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Annotator9.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DjjB_R1-0JKE"},"source":["### Mount Google Drive"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WfsZ7x2snpEX","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1599021244169,"user_tz":240,"elapsed":27777,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"e6a42e72-15a6-4761-a86f-2195a8b29037"},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create and change directory to workspace folder\n","WORKING_PATH = '/content/drive/My Drive/Github/ml-team1-july2020'\n","%cd {WORKING_PATH}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Github/ml-team1-july2020\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_BsOJZeHMtJM"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y2rqfI_jMrhJ","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599021292983,"user_tz":240,"elapsed":4517,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"e92f4068-9ea9-4e62-a87b-7f03124aac94"},"source":["# Add other folders to the system path\n","import sys\n","sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/TagPredictor')\n","sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/ManualTagger')\n","\n","# Import component classes in other folders\n","#from TagPredictor.classifier import Classifier\n","#from TagPredictor.classifier_SVM import Classifier_SVM\n","from TagPredictor.MultilabelClassifier_SVM import MultilabelClassifier_SVM\n","from TagPredictor.TagPredictor import TagPredictor\n","from ManualTagger.ManualTagger import ManualTagger\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import re\n","import ast\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import hamming_loss\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","# Set Pandas display options\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', 20)\n","pd.set_option('display.width', None)\n","pd.set_option('display.expand_frame_repr', False)   # Disable wrapping\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vPwHSTJ04Ud4"},"source":["### Class Definition"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7-bLE6Q532mo","colab":{}},"source":["'''\n","@file       Annotator.ipynb\n","@date       2020/08/03\n","@brief      Top level class that defines the annotation tool and active learning algorithm\n","'''\n","\n","\n","'''\n","@brief  NLP classification annotation tool\n","'''\n","class Annotator:\n","    groundTruthDB = None            # Pandas dataframe of all data with ground truth labels\n","    labeledDB = None                # Pandas dataframe of labeled data\n","    unlabeledDB = None              # Pandas dataframe of unlabeled data\n","\n","    tagPredictor = None             # TagPredictor object\n","    manualTagger = None             # ManualTagger object\n","\n","    confidenceThreshold = 0.96       # Prediction confidence threshold to determine if a topic should be passed to ManualTagger\n","\n","\n","    def __init__(self, datafile):\n","        # Create databases\n","        self.groundTruthDB, self.labeledDB, self.unlabeledDB = self.createDatabases(datafile)\n","\n","        # Set up ManualTagger\n","        self.manualTagger = ManualTagger(self.groundTruthDB)\n","    \n","\n","    '''\n","    @brief      Performs preprocessing and cleaning on a sentence\n","    @param      text    String that contains the raw sentence\n","    @return     text    String that contains the cleaned sentence\n","    '''\n","    def cleanText(self, text):\n","        # Function that checks if all characters in a string are ASCII\n","        def is_ascii(s):\n","            return all(ord(c) < 128 for c in s)\n","        \n","        # Remove URLs\n","        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n","\n","        # Replace newline and tab characters with spaces\n","        text = text.replace('\\n', ' ')\n","        text = text.replace('\\t', ' ')\n","\n","        # Convert all letters to lowercase\n","        text = text.lower()\n","\n","        # Split feature string into a list to perform processing on each word\n","        wordList = text.split()\n","\n","        # Remove all stop words\n","        #stop_words = set(stopwords.words('english'))\n","        #wordList = [word for word in wordList if not word in stop_words]\n","\n","        # Remove all words to contain non-ASCII characters\n","        wordList = [word for word in wordList if is_ascii(word)]\n","\n","        # Remove all leading/training punctuation, except for '$'\n","        punctuation = '!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","        wordList = [word.strip(punctuation) for word in wordList]\n","\n","        # Reconstruct text\n","        text = ' '.join(wordList)\n","\n","        return text\n","\n","\n","    '''\n","    @brief      Loads data from CSV files into Pandas dataframes and performs cleanText() on all columns\n","    @param      datafile        CSV file with all data\n","    @return     groundTruthDB   Pandas dataframe of all data with ground truth labels\n","    @return     labeledDB       Pandas dataframe of the labeled data\n","    @return     unlabeledDB     Pandas dataframe of the unlabeled data\n","    '''\n","    def createDatabases(self, datafile):\n","        # Load CSV file as ground truth database\n","        groundTruthDB = pd.read_csv(datafile)\n","\n","        # Combine topic title and leading comment columns\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Topic Title'] + groundTruthDB['Leading Comment']\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Bag_of_Words'].str.strip().str.replace('   ', ' ').str.replace('  ', ' ')\n","\n","        # Delete unused columns\n","        groundTruthDB = groundTruthDB.drop(columns=['Topic Title', 'Leading Comment', 'Unnamed: 0'])\n","        \n","        # Apply cleanText() to the bag of words\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Bag_of_Words'].apply(lambda x: self.cleanText(x))\n","\n","        # Code to duplicate multi-tag topics if necessary\n","        '''\n","        #create an offset value\n","        offset = 0\n","        #the total number of unique comments\n","        total = len(groundTruthDB)\n","        for index, entry in enumerate(groundTruthDB['Bag_of_Words']):\n","            #create a duplicate if post has multiple tags\n","            tag_list = ast.literal_eval(groundTruthDB.loc[index, 'Tags'])\n","            text = groundTruthDB.loc[index,'Bag_of_Words']\n","            while (isinstance(tag_list, list) and len(tag_list) > 1):\n","                #print(index)\n","                #sets the tag for the duplicate to a string\n","                groundTruthDB.loc[total+offset, 'Tags'] = tag_list.pop()\n","                #Adds the duplicate to the end of the pandas dataframe\n","                groundTruthDB.loc[total+offset, 'Bag_of_Words'] = text\n","                offset = offset + 1\n","            #Changes the first tag to a string\n","            if (len(tag_list) == 1):\n","                groundTruthDB.loc[index, 'Tags'] = tag_list.pop()\n","            #Changes empty tags from lists to strings\n","            if (isinstance(groundTruthDB.loc[index, 'Tags'], list)):\n","                groundTruthDB.loc[index, 'Tags'] = ''\n","                # Not sure why this element is stored as '[]' instead of ''\n","        '''\n","\n","        # Filter out topics with no tags\n","        groundTruthDB = groundTruthDB[groundTruthDB['Tags'].map(len) > 2]\n","\n","        # Convert Tag column elements from strings to lists and strip leading/trailing punctuation\n","        groundTruthDB['Tags'] = groundTruthDB.Tags.apply(lambda x: [tag.strip(\" '\") for tag in x[1:-1].split(',')])\n","\n","        # Take only a subset of the full dataset, if necessary\n","        #groundTruthDB = groundTruthDB.sample(1000)\n","\n","        # Split ground truth database into labeled and unlabelled databases\n","        unlabeledDB, labeledDB = train_test_split(groundTruthDB, test_size=0.2)\n","\n","        return groundTruthDB, labeledDB, unlabeledDB\n","\n","\n","    '''\n","    @brief      Demonstration function to run the entire annotator application\n","    @param      classifier      Scikit-learn like classifier class to be used\n","    @param      maxIterations   Maximum times to run the active learning loop\n","    @return     None\n","    '''\n","    def runApplication(self, classifier, maxIterations=8):\n","        # Create multilabel binarizer for metric calculations\n","        mlb = MultiLabelBinarizer()\n","\n","        # Set up dictionary to keep track of statistics\n","        statDict = {\n","                    'Active Learning Iteration' :   [],\n","                    'Labeled Database Size'     :   [],\n","                    'Unlabeled Database Size'   :   [],\n","                    'Precision (Tag)'           :   [],\n","                    'Recall (Tag)'              :   [],\n","                    'F1 Score (Tag)'            :   [],\n","                    'Hamming Loss'              :   [],\n","                    'Accuracy (Tag List)'       :   []}\n","\n","        # Set up TagPredictor object\n","        tagPredictor = TagPredictor(classifier, self.labeledDB)\n","\n","        # Train tagPredictor\n","        tagPredictor.train()\n","\n","        # Predict tags for all unlabeled topics\n","        tagList, confidenceList = tagPredictor.predict(self.unlabeledDB['Bag_of_Words'])\n","\n","        # Continue running the active learning loop as long as there are still low-confidence topics\n","        counter = 1\n","        print('Minimum Confidence:', min(confidenceList))\n","        print('Maximum Confidence:', max(confidenceList))\n","        while (any(p < self.confidenceThreshold for p in confidenceList) == True and counter <= maxIterations):\n","            ## Start of statistic logging\n","            labeledDBSize = len(self.labeledDB)\n","            unlabeledDBSize = len(self.unlabeledDB)\n","\n","            # Calculate Hamming Loss and tag accuracy at the tag list level\n","            trueLabelIndicatorMatrix = mlb.fit_transform(self.unlabeledDB['Tags'])\n","            predictedLabelIndicatorMatrix = mlb.transform(tagList)\n","            hammingLoss = hamming_loss(trueLabelIndicatorMatrix, predictedLabelIndicatorMatrix)\n","            accuracy = accuracy_score(trueLabelIndicatorMatrix, predictedLabelIndicatorMatrix)\n","\n","            # Calculate average precision, recall and F1 score at the tag level (not the tag list level)\n","            precisions = np.zeros(trueLabelIndicatorMatrix.shape[0])\n","            recalls = np.zeros(trueLabelIndicatorMatrix.shape[0])\n","            fscores = np.zeros(trueLabelIndicatorMatrix.shape[0])\n","            for i in range(trueLabelIndicatorMatrix.shape[0]):\n","                pArray, rArray, fArray, _ = precision_recall_fscore_support(trueLabelIndicatorMatrix[i], predictedLabelIndicatorMatrix[i])\n","                precisions[i] = pArray[1]\n","                recalls[i] = rArray[1]\n","                fscores[i] = fArray[1]\n","            precision = np.average(precisions)\n","            recall = np.average(recalls)\n","            fscore = np.average(fscores)\n","\n","            # Print out active learning statistics\n","            print('Active Learning Iteration:', counter)\n","            print('Labeled Database Size:', labeledDBSize)\n","            print('Unlabeled Database Size:', unlabeledDBSize)\n","            print('Precision:', precision)\n","            print('Recall:', recall)\n","            print('F1 Score:', fscore)\n","            print('Hamming Loss:', hammingLoss)\n","            print('Accuracy:', accuracy)\n","            \n","            # Append statistics to statDict\n","            statDict['Active Learning Iteration'].append(counter)\n","            statDict['Labeled Database Size'].append(labeledDBSize)\n","            statDict['Unlabeled Database Size'].append(unlabeledDBSize)\n","            statDict['Precision (Tag)'].append(precision)\n","            statDict['Recall (Tag)'].append(recall)\n","            statDict['F1 Score (Tag)'].append(fscore)\n","            statDict['Hamming Loss'].append(hammingLoss)\n","            statDict['Accuracy (Tag List)'].append(accuracy)\n","\n","            ## End of statistics logging\n","            \n","            # Get low-confidence topic indices\n","            lowConfIndices = [i for i in range(len(confidenceList)) if confidenceList[i] < self.confidenceThreshold]\n","\n","            # Pass low-confidence topics to the manual tagger\n","            lowConfTopics = self.unlabeledDB.iloc[lowConfIndices]\n","            labeledTopics = self.manualTagger.run(lowConfTopics)\n","\n","            # Add manually tagged topics to the labeled database\n","            self.labeledDB = pd.concat([self.labeledDB, labeledTopics], join='inner')\n","\n","            # Remove tagged topics from unlabeled database\n","            cond = self.unlabeledDB['Bag_of_Words'].isin(lowConfTopics['Bag_of_Words'])\n","            self.unlabeledDB.drop(self.unlabeledDB[cond].index, inplace=True)\n","\n","            # Exit active learning loop if there are no more topics in the unlabeled database\n","            if (len(self.unlabeledDB) == 0 or counter == maxIterations):\n","                break\n","\n","            # Train tagPredictor with updated labeled database\n","            tagPredictor = TagPredictor(classifier, self.labeledDB)\n","            tagPredictor.train()\n","\n","            # Predict tags for all unlabeled topics\n","            tagList, confidenceList = tagPredictor.predict(self.unlabeledDB['Bag_of_Words'])\n","\n","            counter += 1\n","        \n","        # Save statistics to a CSV file\n","        statDataframe =  pd.DataFrame(statDict)\n","        statDataframe.to_csv('AnnotatorStats_2080_96.csv') \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziCVPa5-pK_0","colab_type":"text"},"source":["### Main"]},{"cell_type":"code","metadata":{"id":"J-DFjkzFpIZM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1599022762721,"user_tz":240,"elapsed":1383392,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"b30da9be-5e80-42c8-ae2e-a2c8955b19ba"},"source":["# Path to CSV datafile\n","datafile = '/content/drive/My Drive/Github/ml-team1-july2020/datasets/MergedDataset.csv'\n","\n","# Instantiate Annotator object\n","annotator = Annotator(datafile)\n","\n","# Run annotation application\n","annotator.runApplication(MultilabelClassifier_SVM)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Started training\n","['beautifulsoup' 'nlp' 'nltk' 'scikit-learn' 'scrapy' 'selenium'\n"," 'selenium-webdriver' 'sentiment-analysis' 'splinter'\n"," 'text-classification' 'text-mining' 'tf-idf' 'web-scraping'\n"," 'word-embedding']\n","Finished training\n","Minimum Confidence: 0.6086307946779089\n","Maximum Confidence: 0.9864829703740498\n","Active Learning Iteration: 1\n","Labeled Database Size: 1452\n","Unlabeled Database Size: 5804\n","Precision: 0.6275844245348036\n","Recall: 0.5941879163795084\n","F1 Score: 0.586302549965541\n","Hamming Loss: 0.07466525548882544\n","Accuracy: 0.38749138525155064\n","Started training\n","['beautifulsoup' 'nlp' 'nltk' 'scikit-learn' 'scrapy' 'selenium'\n"," 'selenium-webdriver' 'sentiment-analysis' 'splinter'\n"," 'text-classification' 'text-mining' 'tf-idf' 'web-scraping'\n"," 'word-embedding']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 2\n","Labeled Database Size: 3678\n","Unlabeled Database Size: 3578\n","Precision: 0.7288988261598659\n","Recall: 0.6994130799329235\n","F1 Score: 0.690590646543693\n","Hamming Loss: 0.055917112512976125\n","Accuracy: 0.5016769144773616\n","Started training\n","['beautifulsoup' 'nlp' 'nltk' 'scikit-learn' 'scrapy' 'selenium'\n"," 'selenium-webdriver' 'sentiment-analysis' 'splinter'\n"," 'text-classification' 'text-mining' 'tf-idf' 'web-scraping'\n"," 'word-embedding']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 3\n","Labeled Database Size: 4235\n","Unlabeled Database Size: 3021\n","Precision: 0.7692816948030453\n","Recall: 0.73733862959285\n","F1 Score: 0.7295928500496524\n","Hamming Loss: 0.04941599281221923\n","Accuracy: 0.5395564382654751\n","Started training\n","['beautifulsoup' 'nlp' 'nltk' 'scikit-learn' 'scrapy' 'selenium'\n"," 'selenium-webdriver' 'sentiment-analysis' 'splinter'\n"," 'text-classification' 'text-mining' 'tf-idf' 'web-scraping'\n"," 'word-embedding']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 4\n","Labeled Database Size: 4382\n","Unlabeled Database Size: 2874\n","Precision: 0.785490605427975\n","Recall: 0.7521746694502436\n","F1 Score: 0.745291115750406\n","Hamming Loss: 0.0469977134904066\n","Accuracy: 0.5563674321503131\n","Started training\n","['beautifulsoup' 'nlp' 'nltk' 'scikit-learn' 'scrapy' 'selenium'\n"," 'selenium-webdriver' 'sentiment-analysis' 'splinter'\n"," 'text-classification' 'text-mining' 'tf-idf' 'web-scraping'\n"," 'word-embedding']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 5\n","Labeled Database Size: 4418\n","Unlabeled Database Size: 2838\n","Precision: 0.7896993187690862\n","Recall: 0.756019497298567\n","F1 Score: 0.7489076814658209\n","Hamming Loss: 0.04656196516661633\n","Accuracy: 0.5560253699788583\n","Started training\n","['beautifulsoup' 'nlp' 'nltk' 'scikit-learn' 'scrapy' 'selenium'\n"," 'selenium-webdriver' 'sentiment-analysis' 'splinter'\n"," 'text-classification' 'text-mining' 'tf-idf' 'web-scraping'\n"," 'word-embedding']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-90b73ac44f94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Run annotation application\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mannotator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunApplication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultilabelClassifier_SVM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-f709788f5b74>\u001b[0m in \u001b[0;36mrunApplication\u001b[0;34m(self, classifier, maxIterations)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;31m# Train tagPredictor with updated labeled database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mtagPredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeledDB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mtagPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;31m# Predict tags for all unlabeled topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Github/ml-team1-july2020/TagPredictor/TagPredictor.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m#Test_X_Tfidf = self.Tfidf_vect.transform(Test_X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_X_Tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Github/ml-team1-july2020/TagPredictor/MultilabelClassifier_SVM.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, Train_X_Tfidf, Train_Y, Test_X_Tfidf, Test_Y)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_X_Tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    168\u001b[0m             delayed(_fit_estimator)(\n\u001b[1;32m    169\u001b[0m                 self.estimator, X, y[:, i], sample_weight)\n\u001b[0;32m--> 170\u001b[0;31m             for i in range(y.shape[1]))\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 random_seed)\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32msklearn/svm/_libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"base matrix class for compressed row and column oriented matrices\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"1clUyBhIpOgR","colab_type":"text"},"source":["### Test Code"]},{"cell_type":"code","metadata":{"id":"Nkgm3CT6hKEW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1598848251633,"user_tz":240,"elapsed":2766,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"00326498-72b7-4c47-af84-14e537f3e8f6"},"source":["# Set up TagPredictor object\n","tagPredictor = TagPredictor(MultilabelClassifier_SVM, annotator.labeledDB)\n","\n","# Train tagPredictor\n","tagPredictor.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Started training\n","[\" 'beautifulsoup'\" \" 'nlp'\" \" 'nltk'\" \" 'scrapy'\" \" 'selenium-webdriver'\"\n"," \" 'sentiment-analysis'\" \" 'splinter'\" \" 'text-classification'\"\n"," \" 'text-mining'\" \" 'tf-idf'\" \" 'web-scraping'\" \" 'word-embedding'\"\n"," \"'beautifulsoup'\" \"'nlp'\" \"'nltk'\" \"'scikit-learn'\" \"'scrapy'\"\n"," \"'selenium'\" \"'selenium-webdriver'\" \"'sentiment-analysis'\" \"'splinter'\"\n"," \"'text-classification'\" \"'text-mining'\" \"'tf-idf'\" \"'web-scraping'\"\n"," \"'word-embedding'\"]\n","Finished training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mEFR66ZImCGN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1598848309031,"user_tz":240,"elapsed":2042,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"e0658fa2-a592-4f11-f19a-219a8ad66a09"},"source":["# Predict tags for all unlabeled topics\n","tagList, confidenceList = tagPredictor.predict(annotator.unlabeledDB['Bag_of_Words'])\n","print(tagList)\n","#print(confidenceList)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(\"'scikit-learn'\",), (\"'text-mining'\",), (\"'tf-idf'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\"'nltk'\",), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'splinter'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'word-embedding'\", \"'tf-idf'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'splinter'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'selenium'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'splinter'\", \"'selenium'\"), (\"'selenium'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\"'nltk'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'text-classification'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'nltk'\", \"'text-mining'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\", \"'splinter'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\"'web-scraping'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\"'web-scraping'\",), (\"'selenium'\", \"'web-scraping'\"), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'selenium'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'selenium'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'web-scraping'\",), (\"'selenium'\",), (\"'nltk'\", \"'scikit-learn'\"), (\"'text-mining'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'web-scraping'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'splinter'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'scrapy'\", \"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'nltk'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'text-mining'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'selenium'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\"'scikit-learn'\",), (\" 'word-embedding'\",), (\"'scrapy'\", \"'web-scraping'\"), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'selenium'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'word-embedding'\", \"'word-embedding'\"), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'scikit-learn'\", \"'tf-idf'\"), (\"'text-mining'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'scikit-learn'\",), (\"'tf-idf'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'nltk'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\"'text-mining'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\", \"'text-classification'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'tf-idf'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'text-mining'\",), (\"'selenium'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\"'scikit-learn'\",), (\" 'scrapy'\", \"'web-scraping'\"), (\"'word-embedding'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'tf-idf'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'splinter'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'scrapy'\", \"'web-scraping'\"), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'scrapy'\", \"'scrapy'\", \"'web-scraping'\"), (\"'scikit-learn'\", \"'splinter'\"), (\"'selenium'\", \"'splinter'\"), (\"'scikit-learn'\", \"'splinter'\"), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'word-embedding'\",), (\"'text-mining'\", \"'tf-idf'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'web-scraping'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\", \"'text-mining'\"), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\", \"'web-scraping'\"), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'text-mining'\",), (\"'nltk'\", \"'text-mining'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'splinter'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'text-mining'\",), (\"'text-classification'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\", \"'web-scraping'\"), (\"'selenium'\",), (\"'web-scraping'\",), (\"'scrapy'\",), (\"'selenium'\",), (\"'selenium'\",), (\" 'text-mining'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\"'splinter'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'text-mining'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\"'scikit-learn'\", \"'tf-idf'\"), (\"'text-classification'\",), (\"'selenium'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'nltk'\", \"'text-mining'\"), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'web-scraping'\",), (\"'text-mining'\",), (\"'nltk'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\", \"'tf-idf'\"), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\"'scrapy'\",), (\"'text-classification'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'web-scraping'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'word-embedding'\", \"'word-embedding'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'tf-idf'\",), (\"'selenium'\",), (\"'text-classification'\",), (\"'text-mining'\", \"'tf-idf'\"), (\"'selenium'\", \"'web-scraping'\"), (\"'selenium'\",), (\"'selenium'\",), (\"'selenium'\",), (\"'tf-idf'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scrapy'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\"'scrapy'\", \"'web-scraping'\"), (\"'tf-idf'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\"'text-mining'\",), (\"'scrapy'\", \"'splinter'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'scrapy'\", \"'scrapy'\"), (\"'scikit-learn'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\"'text-classification'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\", \"'word-embedding'\"), (\" 'selenium-webdriver'\", \"'selenium'\"), (\"'selenium'\", \"'web-scraping'\"), (\"'scrapy'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'text-mining'\",), (\"'scikit-learn'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'web-scraping'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'text-mining'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\" 'beautifulsoup'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\"'nltk'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\", \"'web-scraping'\"), (\" 'beautifulsoup'\",), (\" 'selenium-webdriver'\", \"'selenium'\"), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'word-embedding'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'text-mining'\",), (\" 'splinter'\", \"'selenium'\"), (\"'word-embedding'\",), (\"'selenium'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'web-scraping'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'word-embedding'\",), (\" 'beautifulsoup'\",), (\"'selenium'\", \"'web-scraping'\"), (\"'text-classification'\",), (\"'scrapy'\",), (\"'nltk'\",), (\"'selenium'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'scikit-learn'\",), (\" 'beautifulsoup'\",), (\"'text-mining'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\" 'beautifulsoup'\",), (\"'selenium'\",), (\"'scrapy'\",), (\" 'beautifulsoup'\",)]\n"],"name":"stdout"}]}]}