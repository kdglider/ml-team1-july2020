{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TempAnnotator2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DjjB_R1-0JKE"},"source":["### Mount Google Drive"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WfsZ7x2snpEX","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1598926812745,"user_tz":240,"elapsed":17700,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"bae7f247-0a4d-41a1-a386-479e01935c6c"},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create and change directory to workspace folder\n","WORKING_PATH = '/content/drive/My Drive/Github/ml-team1-july2020'\n","%cd {WORKING_PATH}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Github/ml-team1-july2020\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_BsOJZeHMtJM"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y2rqfI_jMrhJ","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598926816930,"user_tz":240,"elapsed":21861,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"0d261a04-7db1-4585-c79e-3b95b6f9f7de"},"source":["# Add other folders to the system path\n","import sys\n","sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/TagPredictor')\n","sys.path.append('/content/drive/My Drive/Github/ml-team1-july2020/ManualTagger')\n","\n","# Import component classes in other folders\n","#from TagPredictor.classifier import Classifier\n","#from TagPredictor.classifier_SVM import Classifier_SVM\n","from TagPredictor.multilabelclassifier_SVM import MultilabelClassifier_SVM\n","from TagPredictor.TagPredictor import TagPredictor\n","from ManualTagger.ManualTagger import ManualTagger\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import re\n","import ast\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import hamming_loss\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","# Set Pandas display options\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', 20)\n","pd.set_option('display.width', None)\n","pd.set_option('display.expand_frame_repr', False)   # Disable wrapping\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vPwHSTJ04Ud4"},"source":["### Class Definition"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7-bLE6Q532mo","colab":{},"executionInfo":{"status":"ok","timestamp":1598928155624,"user_tz":240,"elapsed":543,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}}},"source":["'''\n","@file       Annotator.ipynb\n","@date       2020/08/03\n","@brief      Top level class that defines the annotation tool and active learning algorithm\n","'''\n","\n","\n","'''\n","@brief  NLP classification annotation tool\n","'''\n","class Annotator:\n","    groundTruthDB = None            # Pandas dataframe of all data with ground truth labels\n","    labeledDB = None                # Pandas dataframe of labeled data\n","    unlabeledDB = None              # Pandas dataframe of unlabeled data\n","\n","    tagPredictor = None             # TagPredictor object\n","    manualTagger = None             # ManualTagger object\n","\n","    confidenceThreshold = 0.95       # Prediction confidence threshold to determine if a topic should be passed to ManualTagger\n","\n","\n","    def __init__(self, datafile):\n","        # Create databases\n","        self.groundTruthDB, self.labeledDB, self.unlabeledDB = self.createDatabases(datafile)\n","\n","        # Set up ManualTagger\n","        self.manualTagger = ManualTagger(self.groundTruthDB)\n","    \n","\n","    '''\n","    @brief      Performs preprocessing and cleaning on a sentence\n","    @param      text    String that contains the raw sentence\n","    @return     text    String that contains the cleaned sentence\n","    '''\n","    def cleanText(self, text):\n","        # Function that checks if all characters in a string are ASCII\n","        def is_ascii(s):\n","            return all(ord(c) < 128 for c in s)\n","        \n","        # Remove URLs\n","        text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n","\n","        # Replace newline and tab characters with spaces\n","        text = text.replace('\\n', ' ')\n","        text = text.replace('\\t', ' ')\n","\n","        # Convert all letters to lowercase\n","        text = text.lower()\n","\n","        # Split feature string into a list to perform processing on each word\n","        wordList = text.split()\n","\n","        # Remove all stop words\n","        #stop_words = set(stopwords.words('english'))\n","        #wordList = [word for word in wordList if not word in stop_words]\n","\n","        # Remove all words to contain non-ASCII characters\n","        wordList = [word for word in wordList if is_ascii(word)]\n","\n","        # Remove all leading/training punctuation, except for '$'\n","        punctuation = '!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","        wordList = [word.strip(punctuation) for word in wordList]\n","\n","        # Reconstruct text\n","        text = ' '.join(wordList)\n","\n","        return text\n","\n","\n","    '''\n","    @brief      Loads data from CSV files into Pandas dataframes and performs cleanText() on all columns\n","    @param      datafile        CSV file with all data\n","    @return     groundTruthDB   Pandas dataframe of all data with ground truth labels\n","    @return     labeledDB       Pandas dataframe of the labeled data\n","    @return     unlabeledDB     Pandas dataframe of the unlabeled data\n","    '''\n","    def createDatabases(self, datafile):\n","        # Load CSV file as ground truth database\n","        groundTruthDB = pd.read_csv(datafile)\n","\n","        # Combine topic title and leading comment columns\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Topic Title'] + groundTruthDB['Leading Comment']\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Bag_of_Words'].str.strip().str.replace('   ', ' ').str.replace('  ', ' ')\n","\n","        # Delete unused columns\n","        groundTruthDB = groundTruthDB.drop(columns=['Topic Title', 'Leading Comment', 'Unnamed: 0'])\n","        \n","        # Apply cleanText() to the bag of words\n","        groundTruthDB['Bag_of_Words'] = groundTruthDB['Bag_of_Words'].apply(lambda x: self.cleanText(x))\n","\n","        # Code to duplicate multi-tag topics if necessary\n","        '''\n","        #create an offset value\n","        offset = 0\n","        #the total number of unique comments\n","        total = len(groundTruthDB)\n","        for index, entry in enumerate(groundTruthDB['Bag_of_Words']):\n","            #create a duplicate if post has multiple tags\n","            tag_list = ast.literal_eval(groundTruthDB.loc[index, 'Tags'])\n","            text = groundTruthDB.loc[index,'Bag_of_Words']\n","            while (isinstance(tag_list, list) and len(tag_list) > 1):\n","                #print(index)\n","                #sets the tag for the duplicate to a string\n","                groundTruthDB.loc[total+offset, 'Tags'] = tag_list.pop()\n","                #Adds the duplicate to the end of the pandas dataframe\n","                groundTruthDB.loc[total+offset, 'Bag_of_Words'] = text\n","                offset = offset + 1\n","            #Changes the first tag to a string\n","            if (len(tag_list) == 1):\n","                groundTruthDB.loc[index, 'Tags'] = tag_list.pop()\n","            #Changes empty tags from lists to strings\n","            if (isinstance(groundTruthDB.loc[index, 'Tags'], list)):\n","                groundTruthDB.loc[index, 'Tags'] = ''\n","                # Not sure why this element is stored as '[]' instead of ''\n","        '''\n","\n","        # Filter out topics with no tags\n","        groundTruthDB = groundTruthDB[groundTruthDB['Tags'].map(len) > 2]\n","\n","        # Convert Tag column elements from strings to lists\n","        groundTruthDB['Tags'] = groundTruthDB.Tags.apply(lambda x: x[1:-1].split(','))\n","\n","        # Take only a subset of the full dataset, if necessary\n","        #groundTruthDB = groundTruthDB.sample(1000)\n","\n","        # Split ground truth database into labeled and unlabelled databases\n","        unlabeledDB, labeledDB = train_test_split(groundTruthDB, test_size=0.2)\n","\n","        return groundTruthDB, labeledDB, unlabeledDB\n","\n","\n","    '''\n","    @brief      Demonstration function to run the entire annotator application\n","    @param      classifier      Scikit-learn like classifier class to be used\n","    @param      maxIterations   Maximum times to run the active learning loop\n","    @return     None\n","    '''\n","    def runApplication(self, classifier, maxIterations=8):\n","        # Create multilabel binarizer for metric calculations\n","        mlb = MultiLabelBinarizer()\n","\n","        # Set up dictionary to keep track of statistics\n","        statDict = {\n","                    'Active Learning Iteration' :   [],\n","                    'Labeled Database Size'     :   [],\n","                    'Unlabeled Database Size'   :   [],\n","                    'Precision (Tag)'           :   [],\n","                    'Recall (Tag)'              :   [],\n","                    'F1 Score (Tag)'            :   [],\n","                    'Hamming Loss'              :   [],\n","                    'Accuracy (Tag List)'       :   []}\n","\n","        # Set up TagPredictor object\n","        tagPredictor = TagPredictor(classifier, self.labeledDB)\n","\n","        # Train tagPredictor\n","        tagPredictor.train()\n","\n","        # Predict tags for all unlabeled topics\n","        tagList, confidenceList = tagPredictor.predict(self.unlabeledDB['Bag_of_Words'])\n","\n","        # Continue running the active learning loop as long as there are still low-confidence topics\n","        counter = 1\n","        print('Minimum Confidence:', min(confidenceList))\n","        print('Maximum Confidence:', max(confidenceList))\n","        while (any(p < self.confidenceThreshold for p in confidenceList) == True and counter <= maxIterations):\n","            ## Start of statistic logging\n","            labeledDBSize = len(self.labeledDB)\n","            unlabeledDBSize = len(self.unlabeledDB)\n","\n","            # Calculate Hamming Loss and tag accuracy at the tag list level\n","            trueLabelIndicatorMatrix = mlb.fit_transform(self.unlabeledDB['Tags'])\n","            predictedLabelIndicatorMatrix = mlb.transform(tagList)\n","            hammingLoss = hamming_loss(trueLabelIndicatorMatrix, predictedLabelIndicatorMatrix)\n","            accuracy = accuracy_score(trueLabelIndicatorMatrix, predictedLabelIndicatorMatrix)\n","\n","            # Calculate average precision, recall and F1 score at the tag level (not the tag list level)\n","            precisions = np.zeros(trueLabelIndicatorMatrix.shape[0])\n","            recalls = np.zeros(trueLabelIndicatorMatrix.shape[0])\n","            fscores = np.zeros(trueLabelIndicatorMatrix.shape[0])\n","            for i in range(trueLabelIndicatorMatrix.shape[0]):\n","                pArray, rArray, fArray, _ = precision_recall_fscore_support(trueLabelIndicatorMatrix[i], predictedLabelIndicatorMatrix[i])\n","                precisions[i] = pArray[1]\n","                recalls[i] = rArray[1]\n","                fscores[i] = fArray[1]\n","            precision = np.average(precisions)\n","            recall = np.average(recalls)\n","            fscore = np.average(fscores)\n","\n","            # Print out active learning statistics\n","            print('Active Learning Iteration:', counter)\n","            print('Labeled Database Size:', labeledDBSize)\n","            print('Unlabeled Database Size:', unlabeledDBSize)\n","            print('Precision:', precision)\n","            print('Recall:', recall)\n","            print('F1 Score:', fscore)\n","            print('Hamming Loss:', hammingLoss)\n","            print('Accuracy:', accuracy)\n","            \n","            # Append statistics to statDict\n","            statDict['Active Learning Iteration'].append(counter)\n","            statDict['Labeled Database Size'].append(labeledDBSize)\n","            statDict['Unlabeled Database Size'].append(unlabeledDBSize)\n","            statDict['Precision (Tag)'].append(precision)\n","            statDict['Recall (Tag)'].append(recall)\n","            statDict['F1 Score (Tag)'].append(fscore)\n","            statDict['Hamming Loss'].append(hammingLoss)\n","            statDict['Accuracy (Tag List)'].append(accuracy)\n","\n","            ## End of statistics logging\n","            \n","            # Get low-confidence topic indices\n","            lowConfIndices = [i for i in range(len(confidenceList)) if confidenceList[i] < self.confidenceThreshold]\n","\n","            # Pass low-confidence topics to the manual tagger\n","            lowConfTopics = self.unlabeledDB.iloc[lowConfIndices]\n","            labeledTopics = self.manualTagger.run(lowConfTopics)\n","\n","            # Add manually tagged topics to the labeled database\n","            self.labeledDB = pd.concat([self.labeledDB, labeledTopics], join='inner')\n","\n","            # Remove tagged topics from unlabeled database\n","            cond = self.unlabeledDB['Bag_of_Words'].isin(lowConfTopics['Bag_of_Words'])\n","            self.unlabeledDB.drop(self.unlabeledDB[cond].index, inplace=True)\n","\n","            # Exit active learning loop if there are no more topics in the unlabeled database\n","            if (len(self.unlabeledDB) == 0):\n","                break\n","\n","            # Train tagPredictor with updated labeled database\n","            tagPredictor = TagPredictor(classifier, self.labeledDB)\n","            tagPredictor.train()\n","\n","            # Predict tags for all unlabeled topics\n","            tagList, confidenceList = tagPredictor.predict(self.unlabeledDB['Bag_of_Words'])\n","\n","            counter += 1\n","        \n","        # Save statistics to a CSV file\n","        statDataframe =  pd.DataFrame(statDict)\n","        statDataframe.to_csv('Annotator_Statistics_2080_95_2.csv') \n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziCVPa5-pK_0","colab_type":"text"},"source":["### Main"]},{"cell_type":"code","metadata":{"id":"J-DFjkzFpIZM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598929999369,"user_tz":240,"elapsed":1841802,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"2aa5373c-6b4b-4405-cd29-da10c19e9162"},"source":["# Path to CSV datafile\n","datafile = '/content/drive/My Drive/Github/ml-team1-july2020/datasets/Team1Dataset.csv'\n","\n","# Instantiate Annotator object\n","annotator = Annotator(datafile)\n","\n","# Run annotation application\n","annotator.runApplication(MultilabelClassifier_SVM)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Started training\n","Finished training\n","Minimum Confidence: 0.9067137524823489\n","Maximum Confidence: 0.9861948510062638\n","Active Learning Iteration: 1\n","Labeled Database Size: 1452\n","Unlabeled Database Size: 5804\n","Precision: 0.4026389846083161\n","Recall: 0.36057029634734666\n","F1 Score: 0.36621869974730065\n","Hamming Loss: 0.06015745109473573\n","Accuracy: 0.24190213645761544\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 2\n","Labeled Database Size: 2515\n","Unlabeled Database Size: 4741\n","Precision: 0.4273008507347254\n","Recall: 0.3826724319763763\n","F1 Score: 0.3912114181255712\n","Hamming Loss: 0.05699057323187254\n","Accuracy: 0.2796878295718203\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 3\n","Labeled Database Size: 2856\n","Unlabeled Database Size: 4400\n","Precision: 0.43\n","Recall: 0.3879734848484848\n","F1 Score: 0.39509090909090905\n","Hamming Loss: 0.056293706293706294\n","Accuracy: 0.28295454545454546\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 4\n","Labeled Database Size: 3002\n","Unlabeled Database Size: 4254\n","Precision: 0.4334743770568876\n","Recall: 0.3930222535652719\n","F1 Score: 0.3996944052656323\n","Hamming Loss: 0.05582980724024448\n","Accuracy: 0.28890456041372825\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 5\n","Labeled Database Size: 3061\n","Unlabeled Database Size: 4195\n","Precision: 0.4309495431068732\n","Recall: 0.3898490266189909\n","F1 Score: 0.39686928883591577\n","Hamming Loss: 0.05593655450628037\n","Accuracy: 0.28772348033373063\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 6\n","Labeled Database Size: 3098\n","Unlabeled Database Size: 4158\n","Precision: 0.43314093314093316\n","Recall: 0.39015151515151514\n","F1 Score: 0.3980920314253647\n","Hamming Loss: 0.055814555814555813\n","Accuracy: 0.28884078884078884\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 7\n","Labeled Database Size: 3122\n","Unlabeled Database Size: 4134\n","Precision: 0.4313417190775681\n","Recall: 0.3902797935816804\n","F1 Score: 0.3972826963393001\n","Hamming Loss: 0.05595251386252838\n","Accuracy: 0.2880986937590711\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n","Active Learning Iteration: 8\n","Labeled Database Size: 3140\n","Unlabeled Database Size: 4116\n","Precision: 0.43209426627793973\n","Recall: 0.3878968253968254\n","F1 Score: 0.3965743440233236\n","Hamming Loss: 0.055944905434701354\n","Accuracy: 0.2891156462585034\n","Started training\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"stream","text":["Finished training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1clUyBhIpOgR","colab_type":"text"},"source":["### Test Code"]},{"cell_type":"code","metadata":{"id":"Nkgm3CT6hKEW","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598927726521,"user_tz":240,"elapsed":931411,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}}},"source":["# Set up TagPredictor object\n","tagPredictor = TagPredictor(MultilabelClassifier_SVM, annotator.labeledDB)\n","\n","# Train tagPredictor\n","tagPredictor.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mEFR66ZImCGN","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598927726523,"user_tz":240,"elapsed":931394,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}}},"source":["# Predict tags for all unlabeled topics\n","tagList, confidenceList = tagPredictor.predict(annotator.unlabeledDB['Bag_of_Words'])\n","print(tagList)\n","#print(confidenceList)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aa44mVOClLFI","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598927726524,"user_tz":240,"elapsed":931365,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}}},"source":["from sklearn.metrics import precision_recall_fscore_support\n","import numpy as np\n","\n","#y_true = np.array([[1,0,1,0,0,0,0,0,0,0],\n","#                   [1,0,1,0,0,0,0,0,0,0]])\n","#y_pred = np.array([[1,1,0,0,0,0,0,0,0,0],\n","#                   [1,1,0,0,0,0,0,0,0,0]])\n","\n","y_true = np.array([0,0,1,0,0,0,0,0,0,0])\n","y_pred = np.array([1,1,0,0,0,0,0,0,0,0])\n","\n","print(precision_recall_fscore_support(y_true, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ydXwJdOYx1L","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598927726525,"user_tz":240,"elapsed":931350,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}}},"source":["dict = {'a':[1,2,3], 'b':[4,5,6]}\n","dict['a'].append(5)\n","print(dict)\n","df = pd.DataFrame(dict)\n","print(df)"],"execution_count":null,"outputs":[]}]}